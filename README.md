# ğŸ“· Image Captioning System [![Django](https://img.shields.io/badge/Django-5.0.7-brightgreen)](https://www.djangoproject.com/) [![TensorFlow](https://img.shields.io/badge/TensorFlow-2.17-orange)](https://www.tensorflow.org/)

A deep learning system that generates descriptive captions for images using ResNet50 + LSTM architecture.

<img src="doggy.jpeg" width="300" />
  
*Generated Caption: "a light brown dog is running through a grassy field."*

## ğŸš€ Key Features
- **Hybrid Architecture**: ResNet50 (CNN) for image features + LSTM for sequence modeling
- **Batch Processing**: Efficient feature extraction (64 images/batch)
- **Text Processing**: 
  - Custom tokenization with start/end sequence markers
  - Dynamic padding up to 82 tokens
- **GPU Acceleration**: Automatic GPU detection & utilization
- **Model Preservation**: 
  - Keras model checkpointing 
  - Tokenizer serialization

## ğŸ§  Model Architecture
```python
# Feature extractor (ResNet50)
inputs1 = Input(shape=(2048,))
fe1 = Dropout(0.5)(inputs1)
fe2 = Dense(256, activation='relu')(fe1)

# Sequence model (LSTM)
inputs2 = Input(shape=(max_length,))
se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
se2 = Dropout(0.5)(se1)
se3 = LSTM(256)(se2)

# Combined decoder
decoder1 = add([fe2, se3])
decoder2 = Dense(256, activation='relu')(decoder1)
outputs = Dense(vocab_size, activation='softmax')(decoder2)
```

## ğŸ› ï¸ Training Details
- **Dataset**: Flickr30K (31,000 images with 5 captions each)
- **Validation**: 80/20 train-test split
- **Optimization**:
  - Early stopping (patience=6)
  - Adam optimizer
  - Categorical crossentropy loss
- **Hardware**: Automatic GPU fallback to CPU

## ğŸ“¥ Getting Started
1. **Dataset Setup**
```bash
mkdir -p flickr30k/Images
wget http://example.com/flickr30k.zip && unzip flickr30k.zip
```

2. **Install Dependencies**
```bash
pip install -r requirements.txt
```

3. **Directory Structure**
```
â”œâ”€â”€ flickr30k/
â”‚   â”œâ”€â”€ Images/
â”‚   â””â”€â”€ captions.txt
â”œâ”€â”€ testing/
â”‚   â”œâ”€â”€ image_captioning_model_finalv2.keras
â”‚   â””â”€â”€ tokenizerfinalv2.pkl
â””â”€â”€ train&test.ipynb
```

## ğŸ§ª Usage Example
```python
from caption_generator import generate_caption

# Initialize with pretrained models
generator = CaptionGenerator(
    model_path='testing/image_captioning_model_finalv2.keras',
    tokenizer_path='testing/tokenizerfinalv2.pkl'
)

# Generate caption for new image
caption = generator.predict('dogggy.jpeg')
print(f"Generated: {caption}")
```

## ğŸ“Š Results
| Metric          | Training | Validation |
|-----------------|----------|------------|
| Loss            | 2.14     | 2.67       |
| Caption Length  | 82       | 82         |
| Vocabulary Size | 8,761    | 8,761      |

## ğŸ”§ Technologies
- **Core Framework**: TensorFlow 2.17.0
- **Keras**: 3.4.1
- **Image Processing**: Pillow 10.4.0
- **NLP**: NLTK 3.8.1

## ğŸ“š Python Libraries
```python
import tensorflow as tf
import pandas as pd
import nltk
from tensorflow.keras.layers import LSTM, Embedding, Dense
from tensorflow.keras.applications import ResNet50
```

## ğŸ“œ License
MIT License - See [LICENSE](LICENSE) for details

**Maintainer**: Nipun Bakshi
*Trained on NVIDIA GeForce RTX 3060*  
*Total training time: ~7 hours*
